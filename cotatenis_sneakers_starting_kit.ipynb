{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "from cotatenis_sneakers.sneaker_dataset import SneakerDataset\n",
    "from cotatenis_sneakers.sneaker_transforms import get_transform, UnNormalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_download_ = False\n",
    "folder = \"data/public\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data download and import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _download_:\n",
    "    os.system(\"python download_data.py\")\n",
    "    os.system(\"data/prepare_data.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = get_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(f\"{folder}/train/train.csv\")\n",
    "train_dataset = SneakerDataset(\n",
    "    train_data, folder=f\"{folder}/train\", device=device, transform=transform\n",
    ")\n",
    "test_data = pd.read_csv(f\"{folder}/test/test.csv\")\n",
    "test_dataset = SneakerDataset(\n",
    "    test_data, folder=f\"{folder}/test\", device=device, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 4, figsize=(10, 10))\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        rint = np.random.randint(train_dataset.data.shape[0])\n",
    "        img, brand = train_dataset.get_untransformed_tuple(rint)\n",
    "        ax[i, j].imshow(img)\n",
    "        ax[i, j].set_title(brand)\n",
    "        ax[i, j].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels distribution\n",
    "brands = train_dataset.labels.value_counts()\n",
    "plt.pie(brands, labels=brands.index, autopct=\"%1.1f%%\")\n",
    "plt.title(\"Brands distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"All images have the same dimensions:\",\n",
    "    all(\n",
    "        [\n",
    "            train_dataset.get_untransformed_tuple(i)[0].size\n",
    "            == train_dataset.get_untransformed_tuple(0)[0].size\n",
    "            for i in range(len(train_dataset))\n",
    "        ]\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [\n",
    "    str(train_dataset.get_untransformed_tuple(i)[0].size)\n",
    "    for i in range(len(train_dataset))\n",
    "]\n",
    "\n",
    "\n",
    "sizes = pd.Series(sizes).value_counts()\n",
    "\n",
    "\n",
    "plt.bar(sizes.index, sizes.values)\n",
    "\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "\n",
    "plt.title(\"Images dimensions distribution\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing is done thanks to the `transform` parameter of the sneaker dataset. You can find the details of each step in `cotanis_sneakers/sneaker_transforms.py`. We pad images so they are the same size, and also normalise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(test_dataset.get_untransformed_tuple(752)[0])\n",
    "axs[0].set_title(\"Original image\")\n",
    "\n",
    "axs[1].imshow(F.to_pil_image(test_dataset[752][0]))\n",
    "axs[1].set_title(\"Transformed image\")\n",
    "\n",
    "unnorm = UnNormalize()\n",
    "axs[2].imshow(F.to_pil_image(unnorm(test_dataset[752][0])))\n",
    "axs[2].set_title(\"Transformed image\\nwith denormalisation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=32, shuffle=False, pin_memory=True\n",
    "    )\n",
    "else:\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pretrained model for image classification\n",
    "model = torch.hub.load(\"pytorch/vision\", \"resnet50\", weights=\"IMAGENET1K_V2\")\n",
    "model.fc = torch.nn.Linear(2048, 3)\n",
    "model = model.to(device)\n",
    "\n",
    "model.eval()\n",
    "correct = 0\n",
    "total_correct = 0\n",
    "total = 0\n",
    "\n",
    "\n",
    "print_every = 10\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "\n",
    "        if i % print_every == 0 and i != 0:\n",
    "            print(f\"Iteration {i}, accuracy: {correct / total}\")\n",
    "            total_correct += correct\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "\n",
    "print(\"Total correct\", total_correct)\n",
    "print(\"Total images\", test_dataset.data.shape[0])\n",
    "accuracy = total_correct / test_dataset.data.shape[0]\n",
    "print(f\"Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cotatenis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
